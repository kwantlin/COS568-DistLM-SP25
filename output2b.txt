kw2960@node-0:/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25$   python3 run_glue.py \
  --model_type bert \
  --model_name_or_path bert-base-cased \
  --task_name $TASK_NAME \
  --do_train \
  --do_eval \
  --data_dir $GLUE_DIR/$TASK_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 16 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/RTE2b/ \
  --overwrite_output_dir \
  --master_ip 128.105.144.209 \
  --master_port 12355 \
  --world_size 4 \
  --local_rank $rank \
  --sync_method all_reduce 
Initialized process group: rank 0 out of 4
03/26/2025 16:08:30 - WARNING - __main__ -   Process rank: 0, device: cpu, distributed training: True, 16-bits training: False
03/26/2025 16:08:30 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /users/kw2960/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/26/2025 16:08:30 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

03/26/2025 16:08:31 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /users/kw2960/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/26/2025 16:08:31 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /users/kw2960/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
/users/kw2960/.local/lib/python3.10/site-packages/pytorch_transformers/modeling_utils.py:539: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(resolved_archive_file, map_location='cpu')
03/26/2025 16:08:39 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
03/26/2025 16:08:39 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
03/26/2025 16:08:39 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/proj/cos568proj2-PG0/glue_data/RTE', model_type='bert', model_name_or_path='bert-base-cased', task_name='rte', output_dir='/tmp/RTE2b/', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=True, do_lower_case=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=0, master_ip='128.105.144.209', master_port='12355', world_size=4, sync_method='all_reduce', device=device(type='cpu'), n_gpu=1, output_mode='classification')
03/26/2025 16:08:39 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_train_bert-base-cased_128_rte
/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py:321: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(cached_features_file)
03/26/2025 16:08:51 - INFO - __main__ -   ***** Running training *****
03/26/2025 16:08:51 - INFO - __main__ -     Num examples = 2490
03/26/2025 16:08:51 - INFO - __main__ -     Num Epochs = 3
03/26/2025 16:08:51 - INFO - __main__ -     Instantaneous batch size per device = 16
03/26/2025 16:08:51 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64
03/26/2025 16:08:51 - INFO - __main__ -     Gradient Accumulation steps = 1
03/26/2025 16:08:51 - INFO - __main__ -     Total optimization steps = 117
Epoch:   0%|                                                                                                                                                       | 0/3 [00:00<?, ?it/sM
inibatch 0, Loss: 0.9412831664085388                                                                                                                              | 0/39 [00:00<?, ?it/s]
/users/kw2960/.local/lib/python3.10/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:
        add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
        add_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
                                                                                                                                                                                        M
inibatch 1, Loss: 0.8928709030151367                                                                                                                      | 1/39 [00:05<03:36,  5.69s/it]
                                                                                                                                                                                        M
inibatch 2, Loss: 0.6501649022102356                                                                                                                      | 2/39 [00:09<02:57,  4.80s/it]
                                                                                                                                                                                        M
inibatch 3, Loss: 0.6098117232322693                                                                                                                      | 3/39 [00:13<02:41,  4.48s/it]
                                                                                                                                                                                        M
inibatch 4, Loss: 0.7456605434417725                                                                                                                      | 4/39 [00:17<02:27,  4.21s/it]
Iteration: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [02:18<00:00,  3.56s/it]
03/26/2025 16:11:09 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte███████| 39/39 [02:18<00:00,  3.32s/it]
/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py:321: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(cached_features_file)
03/26/2025 16:11:09 - INFO - __main__ -   ***** Running evaluation  *****
03/26/2025 16:11:09 - INFO - __main__ -     Num examples = 277
03/26/2025 16:11:09 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  2.99it/s]
03/26/2025 16:11:21 - INFO - __main__ -   ***** Eval results  *****██████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  3.25it/s]
03/26/2025 16:11:21 - INFO - __main__ -     acc = 0.5956678700361011
Epoch:  33%|███████████████████████████████████████████████▎                                                                                              | 1/3 [02:30<05:01, 150.68s/itM
inibatch 0, Loss: 0.6282606720924377                                                                                                                              | 0/39 [00:00<?, ?it/s]
                                                                                                                                                                                        M
inibatch 1, Loss: 0.6907647252082825                                                                                                                      | 1/39 [00:03<02:30,  3.96s/it]
                                                                                                                                                                                        M
inibatch 2, Loss: 0.7359381318092346                                                                                                                      | 2/39 [00:07<02:11,  3.57s/it]
                                                                                                                                                                                        M
inibatch 3, Loss: 0.6631338596343994                                                                                                                      | 3/39 [00:10<02:02,  3.41s/it]
                                                                                                                                                                                        M
inibatch 4, Loss: 0.5752015113830566                                                                                                                      | 4/39 [00:13<01:56,  3.32s/it]
Iteration: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [02:07<00:00,  3.27s/it]
03/26/2025 16:13:29 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte███████| 39/39 [02:07<00:00,  3.28s/it]
03/26/2025 16:13:29 - INFO - __main__ -   ***** Running evaluation  *****
03/26/2025 16:13:29 - INFO - __main__ -     Num examples = 277
03/26/2025 16:13:29 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  2.99it/s]
03/26/2025 16:13:41 - INFO - __main__ -   ***** Eval results  *****██████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  3.25it/s]
03/26/2025 16:13:41 - INFO - __main__ -     acc = 0.6209386281588448
Epoch:  67%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 2/3 [04:50<02:24, 144.02s/itM
inibatch 0, Loss: 0.5340057611465454                                                                                                                              | 0/39 [00:00<?, ?it/s]
                                                                                                                                                                                        M
inibatch 1, Loss: 0.6012275218963623                                                                                                                      | 1/39 [00:03<02:19,  3.67s/it]
                                                                                                                                                                                        M
inibatch 2, Loss: 0.5779781341552734                                                                                                                      | 2/39 [00:06<02:05,  3.40s/it]
                                                                                                                                                                                        M
inibatch 3, Loss: 0.5193859934806824                                                                                                                      | 3/39 [00:10<01:58,  3.29s/it]
                                                                                                                                                                                        M
inibatch 4, Loss: 0.46332868933677673                                                                                                                     | 4/39 [00:13<01:53,  3.25s/it]
Iteration: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [02:04<00:00,  3.20s/it]
03/26/2025 16:15:45 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte███████| 39/39 [02:04<00:00,  3.13s/it]
03/26/2025 16:15:45 - INFO - __main__ -   ***** Running evaluation  *****
03/26/2025 16:15:45 - INFO - __main__ -     Num examples = 277
03/26/2025 16:15:45 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  3.01it/s]
03/26/2025 16:15:57 - INFO - __main__ -   ***** Eval results  *****██████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  3.27it/s]
03/26/2025 16:15:57 - INFO - __main__ -     acc = 0.628158844765343
Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [07:06<00:00, 142.14s/it]
03/26/2025 16:15:57 - INFO - __main__ -    global_step = 117, average loss = 0.6138110611683283
03/26/2025 16:15:57 - INFO - __main__ -   Saving model checkpoint to /tmp/RTE2b/
03/26/2025 16:15:58 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py", line 526, in <module>
[rank0]:     main()
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py", line 523, in main
[rank0]:     evaluate(args, model, tokenizer, prefix="")
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py", line 250, in evaluate
[rank0]:     eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py", line 344, in load_and_cache_examples
[rank0]:     torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache
[rank0]:   File "/users/kw2960/.local/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/users/kw2960/.local/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4164, in barrier
[rank0]:     work.wait()
[rank0]: RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [128.105.144.213]:53447