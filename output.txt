kw2960@node-0:/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25$ python3 run_glue.py \
  --model_type bert \
  --model_name_or_path bert-base-cased \
  --task_name $TASK_NAME \
  --do_train \
  --do_eval \
  --data_dir $GLUE_DIR/$TASK_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 64 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$TASK_NAME/ \
  --overwrite_output_dir
03/26/2025 14:09:08 - WARNING - __main__ -   Process rank: -1, device: cpu, distributed training: False, 16-bits training: False
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpdufyfbce
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 433/433 [00:00<00:00, 806811.92B/s]
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpdufyfbce to cache at /users/kw2960/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   creating metadata file for /users/kw2960/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpdufyfbce
03/26/2025 14:09:09 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /users/kw2960/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/26/2025 14:09:09 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp48tzxbm3
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 213450/213450 [00:00<00:00, 2832671.07B/s]
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp48tzxbm3 to cache at /users/kw2960/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   creating metadata file for /users/kw2960/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp48tzxbm3
03/26/2025 14:09:09 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /users/kw2960/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/26/2025 14:09:09 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpsr_ewft9
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 435779157/435779157 [00:10<00:00, 40760972.05B/s]
03/26/2025 14:09:20 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpsr_ewft9 to cache at /users/kw2960/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/26/2025 14:09:21 - INFO - pytorch_transformers.file_utils -   creating metadata file for /users/kw2960/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
03/26/2025 14:09:21 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpsr_ewft9
03/26/2025 14:09:21 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /users/kw2960/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
/users/kw2960/.local/lib/python3.10/site-packages/pytorch_transformers/modeling_utils.py:539: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(resolved_archive_file, map_location='cpu')
03/26/2025 14:09:30 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
03/26/2025 14:09:30 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
03/26/2025 14:09:30 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/proj/cos568proj2-PG0/glue_data/RTE', model_type='bert', model_name_or_path='bert-base-cased', task_name='rte', output_dir='/tmp/RTE/', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=True, do_lower_case=False, per_device_train_batch_size=64, per_device_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, device=device(type='cpu'), n_gpu=0, output_mode='classification')
03/26/2025 14:09:30 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_train_bert-base-cased_128_rte
/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(cached_features_file)
03/26/2025 14:09:32 - INFO - __main__ -   ***** Running training *****
03/26/2025 14:09:32 - INFO - __main__ -     Num examples = 2490
03/26/2025 14:09:32 - INFO - __main__ -     Num Epochs = 3
03/26/2025 14:09:32 - INFO - __main__ -     Instantaneous batch size per device = 64
03/26/2025 14:09:32 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64
03/26/2025 14:09:32 - INFO - __main__ -     Gradient Accumulation steps = 1
03/26/2025 14:09:32 - INFO - __main__ -     Total optimization steps = 117
Epoch:   0%|                                                                                                                                                                            | 0/3 [00:00<?, ?it/sM
inibatch 0, Loss: 0.7691709399223328                                                                                                                                                   | 0/39 [00:00<?, ?it/s]
/users/kw2960/.local/lib/python3.10/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:
        add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
        add_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
                                                                                                                                                                                                             M
inibatch 1, Loss: 0.7817338705062866                                                                                                                                           | 1/39 [00:14<09:22, 14.80s/it]
                                                                                                                                                                                                             M
inibatch 2, Loss: 0.6885838508605957                                                                                                                                           | 2/39 [00:25<07:38, 12.40s/it]
                                                                                                                                                                                                             M
inibatch 3, Loss: 0.7662752866744995                                                                                                                                           | 3/39 [00:34<06:34, 10.96s/it]
                                                                                                                                                                                                             M
inibatch 4, Loss: 0.7341869473457336                                                                                                                                           | 4/39 [00:43<05:56, 10.19s/it]
Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [05:32<00:00,  8.52s/it]
03/26/2025 14:15:04 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte████████████████████████████| 39/39 [05:32<00:00,  8.14s/it]
/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/run_glue.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(cached_features_file)
03/26/2025 14:15:04 - INFO - __main__ -   ***** Running evaluation  *****
03/26/2025 14:15:04 - INFO - __main__ -     Num examples = 277
03/26/2025 14:15:04 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:12<00:00,  2.84it/s]
03/26/2025 14:15:16 - INFO - __main__ -   ***** Eval results  *****███████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:12<00:00,  3.10it/s]
03/26/2025 14:15:16 - INFO - __main__ -     acc = 0.628158844765343
Epoch:  33%|██████████████████████████████████████████████████████▎                                                                                                            | 1/3 [05:44<11:29, 344.57s/itM
inibatch 0, Loss: 0.6644679307937622                                                                                                                                                   | 0/39 [00:00<?, ?it/s]
                                                                                                                                                                                                             M
inibatch 1, Loss: 0.6756858229637146                                                                                                                                           | 1/39 [00:08<05:19,  8.42s/it]
                                                                                                                                                                                                             M
inibatch 2, Loss: 0.6787503361701965                                                                                                                                           | 2/39 [00:16<05:12,  8.44s/it]
                                                                                                                                                                                                             M
inibatch 3, Loss: 0.6386573910713196                                                                                                                                           | 3/39 [00:25<05:04,  8.45s/it]
                                                                                                                                                                                                             M
inibatch 4, Loss: 0.6459658741950989                                                                                                                                           | 4/39 [00:33<04:52,  8.36s/it]
Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [05:15<00:00,  8.08s/it]
03/26/2025 14:20:31 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte████████████████████████████| 39/39 [05:15<00:00,  7.74s/it]
03/26/2025 14:20:31 - INFO - __main__ -   ***** Running evaluation  *****
03/26/2025 14:20:31 - INFO - __main__ -     Num examples = 277
03/26/2025 14:20:31 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  2.93it/s]
03/26/2025 14:20:43 - INFO - __main__ -   ***** Eval results  *****███████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  3.19it/s]
03/26/2025 14:20:43 - INFO - __main__ -     acc = 0.6498194945848376
Epoch:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 2/3 [11:11<05:34, 334.29s/itM
inibatch 0, Loss: 0.6291962265968323                                                                                                                                                   | 0/39 [00:00<?, ?it/s]
                                                                                                                                                                                                             M
inibatch 1, Loss: 0.5876450538635254                                                                                                                                           | 1/39 [00:07<05:02,  7.96s/it]
                                                                                                                                                                                                             M
inibatch 2, Loss: 0.5566092729568481                                                                                                                                           | 2/39 [00:15<04:54,  7.95s/it]
                                                                                                                                                                                                             M
inibatch 3, Loss: 0.5800958275794983                                                                                                                                           | 3/39 [00:23<04:47,  7.99s/it]
                                                                                                                                                                                                             M
inibatch 4, Loss: 0.5509496331214905                                                                                                                                           | 4/39 [00:31<04:38,  7.96s/it]
Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [05:12<00:00,  8.01s/it]
03/26/2025 14:25:56 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte████████████████████████████| 39/39 [05:12<00:00,  7.77s/it]
03/26/2025 14:25:56 - INFO - __main__ -   ***** Running evaluation  *****
03/26/2025 14:25:56 - INFO - __main__ -     Num examples = 277
03/26/2025 14:25:56 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  2.93it/s]
03/26/2025 14:26:08 - INFO - __main__ -   ***** Eval results  *****███████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:11<00:00,  3.19it/s]
03/26/2025 14:26:08 - INFO - __main__ -     acc = 0.6209386281588448
Epoch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [16:36<00:00, 332.02s/it]
03/26/2025 14:26:08 - INFO - __main__ -    global_step = 117, average loss = 0.6365272191345183
03/26/2025 14:26:08 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte
03/26/2025 14:26:08 - INFO - __main__ -   ***** Running evaluation  *****
03/26/2025 14:26:08 - INFO - __main__ -     Num examples = 277
03/26/2025 14:26:08 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:12<00:00,  2.88it/s]
03/26/2025 14:26:20 - INFO - __main__ -   ***** Eval results  *****
03/26/2025 14:26:20 - INFO - __main__ -     acc = 0.6209386281588448