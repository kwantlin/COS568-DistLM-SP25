kw2960@node-0:/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/task2a$ python3 run_glue.py \
  --model_type bert \
  --model_name_or_path bert-base-cased \
  --task_name $TASK_NAME \
  --do_train \
  --do_eval \
  --data_dir $GLUE_DIR/$TASK_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 16 \
  --learning_rate 2e-5 \
  --num_train_epochs 1 \
  --output_dir /tmp/RTE2a/ \
  --overwrite_output_dir \
  --master_ip 10.10.1.2 \
  --master_port 12355 \
  --world_size 4 \
  --local_rank $rank 
Initialized process group: rank 0 out of 4
03/28/2025 07:20:37 - WARNING - __main__ -   Process rank: 0, device: cpu, distributed training: True, 16-bits training: False
03/28/2025 07:20:37 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /users/kw2960/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
03/28/2025 07:20:37 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

03/28/2025 07:20:38 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /users/kw2960/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
03/28/2025 07:20:38 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /users/kw2960/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
/users/kw2960/.local/lib/python3.10/site-packages/pytorch_transformers/modeling_utils.py:539: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(resolved_archive_file, map_location='cpu')
03/28/2025 07:20:46 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
03/28/2025 07:20:46 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
03/28/2025 07:20:46 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='/proj/cos568proj2-PG0/glue_data/RTE', model_type='bert', model_name_or_path='bert-base-cased', task_name='rte', output_dir='/tmp/RTE2a/', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=True, do_lower_case=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=0, master_ip='10.10.1.2', master_port='12355', world_size=4, device=device(type='cpu'), n_gpu=0, output_mode='classification')
03/28/2025 07:20:46 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_train_bert-base-cased_128_rte
/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/task2a/run_glue.py:347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(cached_features_file)
03/28/2025 07:20:57 - INFO - __main__ -   ***** Running training *****
03/28/2025 07:20:57 - INFO - __main__ -     Num examples = 2490
03/28/2025 07:20:57 - INFO - __main__ -     Num Epochs = 1
03/28/2025 07:20:57 - INFO - __main__ -     Instantaneous batch size per device = 16
03/28/2025 07:20:57 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64
03/28/2025 07:20:57 - INFO - __main__ -     Gradient Accumulation steps = 1
03/28/2025 07:20:57 - INFO - __main__ -     Total optimization steps = 39
Epoch:   0%|                                                                                                                                                                            | 0/1 [00:00<?, ?it/sM
inibatch 0, Loss: 0.9412831664085388                                                                                                                                                   | 0/39 [00:00<?, ?it/s]
/users/kw2960/.local/lib/python3.10/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:
        add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
        add_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
                                                                                                                                                                                                             M
inibatch 1, Loss: 0.8928709030151367                                                                                                                                           | 1/39 [00:08<05:40,  8.96s/it]
                                                                                                                                                                                                             M
inibatch 2, Loss: 0.6499287486076355                                                                                                                                           | 2/39 [00:16<04:50,  7.86s/it]
                                                                                                                                                                                                             M
inibatch 3, Loss: 0.6082603931427002                                                                                                                                           | 3/39 [00:23<04:31,  7.53s/it]
                                                                                                                                                                                                             M
inibatch 4, Loss: 0.748154878616333                                                                                                                                            | 4/39 [00:30<04:17,  7.35s/it]
Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [04:11<00:00,  6.46s/it]
03/28/2025 07:25:09 - INFO - __main__ -   Average time per iteration (excluding first): 6.3887 seconds████████████████████████████████████████████████████████████████████████| 39/39 [04:11<00:00,  6.25s/it]
03/28/2025 07:25:09 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte
/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/task2a/run_glue.py:347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  features = torch.load(cached_features_file)
03/28/2025 07:25:09 - INFO - __main__ -   ***** Running evaluation  *****
03/28/2025 07:25:09 - INFO - __main__ -     Num examples = 277
03/28/2025 07:25:09 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:13<00:00,  2.60it/s]
03/28/2025 07:25:23 - INFO - __main__ -   ***** Eval results  *****███████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:13<00:00,  2.90it/s]
03/28/2025 07:25:23 - INFO - __main__ -     acc = 0.5956678700361011
Epoch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:25<00:00, 265.33s/it]
03/28/2025 07:25:23 - INFO - __main__ -    global_step = 39, average loss = 0.6936845275071951
03/28/2025 07:25:23 - INFO - __main__ -   Loading features from cached file /proj/cos568proj2-PG0/glue_data/RTE/cached_dev_bert-base-cased_128_rte
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/task2a/run_glue.py", line 534, in <module>
[rank0]:     main()
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/task2a/run_glue.py", line 531, in main
[rank0]:     evaluate(args, model, tokenizer, prefix="")
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/task2a/run_glue.py", line 276, in evaluate
[rank0]:     eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)
[rank0]:   File "/proj/cos568proj2-PG0/groups/kw2960/COS568-DistLM-SP25/task2a/run_glue.py", line 370, in load_and_cache_examples
[rank0]:     torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache
[rank0]:   File "/users/kw2960/.local/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/users/kw2960/.local/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4164, in barrier
[rank0]:     work.wait()
[rank0]: RuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:525] Read error [128.105.144.221]:20646: Connection reset by peer